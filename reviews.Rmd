---
title          : "Why does preregistration increase the persuasiveness of evidence? A Bayesian rationalization"
authors        : "Aaron Peikert, Maximilian S. Ernst & Andreas M. Brandmaier"
journal        : "Meta-Psychology"
manuscript-id  : "4139"
cutout-quotes  : true
class          : "draft"
output         : papaja::revision_letter_pdf
manuscript-src : manuscript.Rmd
---

Dear Dr. Rickard Carlsson,

thank you very much for taking the time to consider our manuscript for publication at _`r rmarkdown::metadata$journal`_.
In the following we address your and each reviewers'/editors' concerns point-by-point.

All changes of the manuscript since this review can be viewed on GitHub:

[https://github.com/aaronpeikert/bayes-prereg/compare/v0.2.0...main](https://github.com/aaronpeikert/bayes-prereg/compare/v0.2.0...main#diff-a9a4aad3fa8c9c10c5404b632bc3a01a25d2d8430eb932bc35c76769963e4b70)

We have updated the preprint:

<https://osf.io/preprints/psyarxiv/cs8wb>

 OSF (<https://osf.io/nc5g8/>) and Zenodo (<https://doi.org/10.5281/zenodo.7648471>) are mirroring our GitHub repo (<https://github.com/aaronpeikert/bayes-prereg>).

Concerning Daniel Laken's review, we took the liberty to provide a high level summary of his main points because they are repeated across different subdiscussions throughout his letter.
We are grateful for his comments and revised the manuscript in response to his summary of suggestions (RC \@ref(pt:change-start) to RC \@ref(pt:change-end)).

Due to a technical issue, the citations in the manuscript excerpts in this letter do not work correctly (they are correct in the manuscript, however).

# Reviewer 1: Daniel Lakens

We begin with a summarizing our understanding of Daniel Lakens philosophical position, his main points of criticisim and our general take on how to address these issues.
Thereafter, we will provide point-by-point responses in the usual style of response letters.
Lakens and our own positions on preregistration fundamentally differ.
They each build upon two long standing traditions in philosophy of science, namely _inductionist_ and _anti-inductionist_, which differ in their core epistemological assumptions.
We believe that a lot of seemingly critical points raised by Daniel Lakens can simply be traced back to differences in these philosophical stances and thus cannot be resolved in the various sub points he raised. 

Integration with Existing Literature:

* Reviewer Concern: The manuscript needs better engagement with existing ideas, particularly those from Mayo’s work on severity.
* Our Response: We expanded our discussion to include more literature that employs a Bayesian rationale and highlighted how our approach differs fundamentally from the concept of severity, situating our work within the broader philosophical landscape that is not limited to (Neo-)Popperian approaches.

Bayesian vs. Frequentist Interpretation:

* Reviewer Concern: The reviewer challenges our Bayesian approach, suggesting it merely re-frames a frequentist argument using the likelihood ratio.
* Our Response: We clarified that our Bayesian approach, while indeed utilizing the likelihood ratio, offers a more explicit and calculable framework for updating beliefs based on evidence---a hallmark of Bayesian reasoning.

Objective vs. Subjective Judgement:

* Reviewer Concern: The reviewer is skeptical about the extent to which preregistration can objectively minimize uncertainty, particularly in exploratory studies, suggesting that our focus on subjective evaluation might downplay the need for more objective control of error rates.
* Our Response: We argued that preregistration reduces uncertainty about theoretical risk, which is inherently a subjective evaluation made by the reader. Bayesian reasoning accommodates the subjectivity in scientific judgment, allowing for rational, individual interpretations of evidence.
We acknowledged the reviewer’s concerns but maintained that the subjective aspect of Bayesian analysis is a strength, not a weakness.

Control vs. Transparency:

* Reviewer Concern: The reviewer agrees that preregistration enhances transparency but believes we overstate its ability to minimize uncertainty, suggesting that error control should be the primary focus.
* Our response: We contend that error control, as traditionally understood in the frequentist framework, assumes fully known random processes, making it difficult to analyse situations with incomplete or absent preregistration.
As a consequence, a severe tester must differentiate error control and transparency, the former supported by formal reasoning, and the latter a necessary addition outside of the core philosophy of severity.
Not so in our Bayesian framework, where transparency maps cleanly onto uncertainty which can be mathematically modeled.

HARKing and Error-Statistical Perspective:

* Reviewer Concern: The manuscript lacked a discussion on HARKing, a practice detrimental to the integrity of hypothesis testing, and the error-statistical perspective emphasizing the importance of severe tests.
* Our Response: We thank the reviewer for raising this issue. We added a dedicated section titled “Hacking, harking, and other harms” to address how our Bayesian framework addresses these concerns.

Type I and Type II error rates:

* Reviewer Concern: The manuscript was perceived as downplaying the importance of statistical power (P(E|H)) in favor of focusing on Type 1 error rates.
* Our Response: The Bayesian rational considers theoretical risk and detectability simultaneously a priori. From a severity perspective, either Type I or Type II, never both, is considered post hoc (i.e., after a test result was obtained from data) depending on the test results and if the research hypothesis is H0 or H1. 

Terminology and Conceptual Clarity:

* Reviewer Concern: The terminology used, particularly "theoretical risk," was seen as potentially confusing and misaligned with existing literature, such as the concept of severity.
* Our Response: We provided additional references and explanations to support our terminology. We believe our terms accurately reflect the concepts within our Bayesian framework, but we differentiated our terminology further from frequentist reasoning.

:::reviewer
Reviewer #1: The authors set out to explain what the primary aim of preregistration is.
I think they are correct that many scientists have hardly thought about this question.
This is not in itself bad - preregistration intuitively makes sense - but it is important to clearly articulate what the aim of preregistration is.
:::

We agree.

:::reviewer
So I applaud the authors for believing this is important.
At the same time, their arguments were, I believe, not always equally convincing, and they do not adequately discuss the existing literature.
:::

We thank the reviewer for pointing out that we failed to better acknowledge opposing philosophical perspectives.
We are now citing Laken's and Mayo's positions and relate them to our perspective.

:::reviewer
The main argument of the authors is straightforward: preregistration reduces uncertainty about the Type 1 error rate, and under the assumption that it is possible to inflate the Type 1 error rate when a study is not preregistered, reducing the uncertainty through preregistration increases the posterior probability of the hypothesis after a test that yields a significant result.
The authors say they provide a 'Bayesian rationalization'.
This is not correct.
:::

We respectfully disagree.
What makes the arguments "Bayesian" is our interpretation of probabilities.
For a Bayesian, they represent degrees of belief.
For a frequentist, they represent (relative) frequencies of events that result from a random process.
This is a subtle but most important distinction.
A Bayesian may assign a probability to a hypothetical world where a paper was p hacked, since probabilities represent their subjective believes.
A frequentist (and thus a severe tester, too), cannot possibly assign a probability here because there is no random process but a single event that happened in the past, so the paper was either p hacked or not.
This is similar to the mistake, in frequentist philosophy, to claim that an effect size found in a given study is with 95% probability inside the confidence interval.
However, a frequentist can only make a statement about an expected values but not about a single case.

:::reviewer
Their rationalization is in essence based on the likelihood ratio.
All else equal, a lower Type 1 error rate means that observing a significant result is relatively more likely when H1 is true.
Of course a higher likelihood ratio also means a higher Bayes factor, which means a higher probability that H1 is true in a Bayesian framework.
:::

Indeed, the math for frequentist inference is identical to Bayesian under the right conditions.
Still, a frequentist would not conclude anything about the probability of H1.
However, our goal is to say something about how preregistration changes how believable researchers should find claims made in the literature.
This necessitates a Bayesian perspective on the process of scientific inquiry (which we adopted) yet it applies to cases of researchers either using frequentist and Bayesian inference in their daily routine.

:::reviewer
But the workhorse in the rationalization that the authors provide is the likelihood ratio.
This does not change when we put a distribution on the Type 1 error rate (as the authors do).
:::

The mere use of the likelihood ratio in the Bayes theorem can not be grounds to reduce Bayesianism to likelihood ratios.

:::reviewer
The distribution (or the move to a Bayesian rationalization) does not add anything (although of course it can be added).
:::

We find that it adds value by making an otherwise implicit process (that is, the process of updating beliefs in the theories) explicit to the point of making it calculable.
To our knowledge, there is no other perspective in the literature that makes this part of the process explicit.
By conceptualising uncertainty with a specific distribution, we rid ourselves of vague notions of transparency, control, or exploration.
This is of utmost importance because only this clarity enables us to effectively navigate the tradeoffs involved.

:::reviewer
Importantly, the likelihood function of course also underlies the uniformly most powerful test in a Neyman-Pearson hypothesis testing framework.
So for a frequentist (and I mean a frequentist not in statistical terms, but also in Popperian anti-Bayesian view on confirmation) the same argument for preregistration plays a role.
:::

We agree that this argument could perhaps be made for a frequentist as well.
However, it is not the math of the likelihood ratio that eludes us; it is how a significant result of a statistical test changes how a frequentist judges a hypothesis.
There is a great, yet unresolved, debate in frequentism about how to do just this.
In a Fisherian approach, no heat is paid to the power of a test; only the p-value is interpreted as the strength of evidence against the null.
In the Neyman-Person paradigm, a decision is to be made about rejecting one or the other hypothesis.
Mayo's severe testing returns to a gradual inference (instead of a decision) by evaluating the extent of evidence in favour of H as the extent to which a test was capable of showing that H is false.
The Bayesian position deals with what agents (here "researchers") ought to believe, assuming they are rational.
However, even here, there is no universally agreed upon function that maps evidence onto belief (indeed, there can not be).
We tried to make this even clearer and more explicit as to why we include several functions (and that our reasoning applies to all possible functions satisfying our assumptions).
The frequentist understanding of probability refers strictly to statistical procedures.
From this position, one must either argue that preregistration is changing the underlying random process/ statistical procedure or trace the impact of preregistration via indirect paths outside of the core philosophy of frequentism.
Mayo's severity perspective is taking one more step in this direction by asserting that belief in a hypothesis is justified to the extent that it survives stringent scrutiny.
However, even this "strong" severity principle is only a proportional statement.
Faced with a specific tradeoff, e.g., is a particular post hoc change of a preregistration beneficial, we must revert to vague notions and "gut" instinct under this philosophy.
Within the Bayesian framework, we may directly ask: What consequences should preregistration have on the beliefs of individual scientists or the research community as a whole?

:::reviewer
The title should not read a Bayesian rationalization, but a likelihood rationalization.
A higher likelihood is not always better, as one can cherry pick a high likelihood ratio.
This is extensively discussed by Mayo (2018).
After p-hacking, we could get a very high likelihood (or Bayes factor) that supports H1 - but it was cherry-picked, and is highly likely to be a fluke.
Therefore, the authors limit themselves to relative likelihoods.
:::

We did not limit ourselves to relative likelihoods.
We explicitly include both the "firmness" and "increase in firmness" perspective.
Since we showed that our justification of preregistration holds for both perspectives, the discussion about specific flaws of the firmness perspective does not concern our argument.  

:::reviewer
They point out that, assuming people only increase their alpha level when p-hacking, preregistration leads to tests with a lower alpha level.
As such, they basically restate Mayo's (2018) argument about the severity of a test.
:::

Let us cite <https://doi.org/10.3758/s13423-022-02069-1> "Nowhere in Mayo’s severity principle or in the specifications discussed below is the riskiness or specificity of a hypothesis, and the predictions it makes, incorporated as a requirement for a severe test (Meehl, 1978; 1986; 1990b, a; Roberts & Pashler, 2000)."

:::reviewer
However, they never make this explicit, and I think this is to the detriment of the paper.
:::

We added a new section that puts the concept of severity into perspective and explicates how our approach differs fundamentally from it:

> Theoretical risk may conceptually be related to the framework of "severity" [@mayoStatisticalInferenceSevere2018; @mayoErrorStatistics2011].
Severity, is a Neopopperian view which asserts that there is evidence for a hypothesis just to the extent that it survives stringent scrutiny.
However, there are crucial differences between the two.
First, our perspective on theoretical risk is not primarily concerned with avoiding inductive reasoning but with subjective changes of belief.
This is important because, while severity is calculable, it remains unclear how severity should be valued, e.g. if an increase in severity from .80 to .81 should be as impressive as from .99 to .999.
Second, severity considerations are mainly after the fact.
Severity, a measure with which we can rule out alternative explanations, can only be calculated after evidence was observed.
This makes it difficult to guide a priori decisions in planning a study, after all severity disregards power, if we observe evidence, and disregards Type I error rate when we do not.
This implies that for a priori balancing Type I and Type II error rate, a researcher must assign a priori probabilities to, for example, the size of an effect.
Since such a move not in line with frequentist rationale there is no guidelines available on how to do this.
Third, we would argue that severity considerations assume full information about how the evidence came about and hence imply axiomatically the need for perfect preregistration.
This comes down to frequentist understanding of probability as the outcome of a well defined random experiment.
When judging a particular study, a frequentist, and hence a severe tester, may not assign probability to the event that the researchers did, for example, p-hack.
The lack of knowledge on the readers side does not turn the p hacking into a random event of which we can calculate the long run frequency aka frequentist probability.
A severe test, hence, must assume that they know the Type I and Type II error rate precisely.
Full transparency, is hence assumed, and we can not imagine many ways except preregistration that get close to this ideal.
This assumptions also makes it hard to deal with less-than-perfect preregistrations and post-hoc changes without relying to principles outside the core philosophy of severity.

> However, there also are communalities, like the strong emphasis on counterfactual consideration (imagining the hypothesis was false), and there are even proposals to reconcile Bayesian and severity considerations [@vandongenBayesianPerspectiveSeverity2023].

:::reviewer
The fact that they subsequently add a Bayesian sauce to the likelihood ratio (by adding a distribution to the Type 1 error rate when a study is not preregistered, instead of simply making some assumption about a point-value of the inflated Type 1 error rate) does not add anything to their argument.
So, I believe the authors have basically attempted to re-invent Mayo's severity argument.
:::

The whole second part of the paper deals with the problem of not being able to reduce the theoretical risk to a point estimate.
The statistical procedure has an objective Type 1 error rate, for which we can come up with a point estimate if we agree on the conditions, which is a starting point for evaluating the theoretical risk of the scientific (!= statistical) procedure.
Since readers do not have access to the whole scientific procedure, they must judge it with uncertainty.
We find it important to disentangle the statistical procedure, the scientific procedure, and the outside perspective of the reader.

:::reviewer
The fact that this is not immediately clear is mainly due to missing discussions of the existing literature.
Point 1: Preregistration can not objectively minimize uncertainty about the Type 1 error rate The main argument of the authors is that risky predictions create persuasive evidence if they turn out to be correct.
Preregistrations are ideal for communicating the theoretical risk researchers took when testing a claim, and communicating this removes uncertainty in the reader about the theoretical risk the authors took.
I think this argument is in the right direction, but it is not fully developed.
I was somewhat disappointed not to see a discussion of my own paper on a conceptual analysis of the value of preregistration, as I had a similar goal as the authors when I wrote my paper, and my arguments are in the same direction, but dare I say, slightly better developed.
:::

Importantly, we agree with Laken's conclusions about preregistration but we fundamentally disagree with how to arrive at these conclusions.
We take a Bayesian/inductionist view, whereas Lakens takes a frequentist/anti-inductionist view, agreeing on the end result but disagreeing on everything that comes before.
While we find it important that readers are aware, how anti-inductionist deal with preregistration, this paper was not meant as an exercise to compare Bayesian justification for preregistration with its frequentist counterpart.

:::reviewer
Maybe everyone thinks this of their own papers, so forgive me if I am too biased, but I will attempt to point out some weaknesses in the current paper, sometimes by citing my own paper.
I hope the authors see how strongly the ideas are related, and forgive me for citing myself.
:::

Of course, we see the relevance of Laken's work; we met in Eindhoven to discuss an early version of the paper.
With respect to how well developed our respective arguments are, we would like to point out that we think that a key part of our argument is the mathematical formulation that enables research to actually compute tradeoffs they face during the research process instead of juggling with vague notions of unquantifiable philosophical principles.

:::reviewer
The authors argue that one interpretation of preregistration is "the traditional application of preregistration to research paradigms that focus on confirmation by maximizing the theoretical risk or, equivalently, by limiting type-I error (when dichotomous decisions about theories are an inferential goal).
I would say it is better to argue preregistration aims to *control* the error rate, not maximize the true negative rate.
The authors then propose their own view: "the objective of preregistration is not the maximization of theoretical risk but rather the minimization of uncertainty about the theoretical risk."
I think the authors are in the right direction, but miss some things.
What the authors get right is that preregistration is about communicating something about the test to others.
:::

This is why we deliberately chose to use the term "uncertainty" and not "control".
Reducing uncertainty directly invokes the concept of communication.
Control also seems to imply that it is a property of the procedure, as it seems curious to suggest that control differs between the reader and the original researcher (we argue that the uncertainty is lower for the latter as they have all the information).

:::reviewer
And the authors are correct that preregistration can remove uncertainty if all readers of a preregistration 1) agree on the analyses plan, and 2) there are no deviations of the preregistration.
:::

We make no such claim.
We explicitly address that readers may differ considerably in how highly they judge the theoretical risk and their confirmation function.

> One feature of the Bayesian framework is the strong emphasis on subjective yet rational judgement.
> Therefore, we assume that researchers will differ significantly in how they value evidence but that by making assumptions about the general process, we can make general statements that apply to all these subjective evaluations.

They may, therefore, very well disagree on the analysis.
We also discuss at length under which circumstances a post hoc change to the preregistration is justified (if the increase in confirmation does offset the negative effects of uncertainty).

:::reviewer
However, the authors already say 2 is not always true, as people often need to deviate.
And 1 is also not true.
This is important, because if 1 and 2 are true, then we can not minimize uncertainty about theoretical risk (as the authors argue the goal of preregistration is), because this uncertainty is based on a subjective evaluation.
If you deviate from an analysis plan because you think after looking at the data that you should have excluded some outliers, but this was not specified in the preregistration, as a reader I am left with an evaluation of whether 1) this analysis is a *better* test, and it reduces the possibility of incorrectly claiming there was an effect, because these outliers might make a null effect significant, while they should have been removed, or 2) this analysis is a *worse* test, and the reader believes the choice for this test without the outliers is driven by the nicer results the researcher observed.
The preregistration does not minimize uncertainty about the Type 1 error rate.
For some readers, the deviation increased the Type 1 error rate, and for others it did not.
:::

This aligns with our reasoning: Any change should increase uncertainty about the theoretical risk.
How much and how this ultimately changes the persuasiveness of the evidence will differ between readers. 

:::reviewer
So what *does* preregistration achieve?
In my own words "Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction." The lower the Type 1 error rate, the higher the capacity of a test to falsify a prediction.
But whether a test, especially after a deviation, has a higher or lower Type 1 error rate, remains a subjective decision.
:::

Agreed. It remains a subjective decision.
As subjective yet rational judgement is such a fundamental property of Bayesian reasoning, we may not have emphasized this enough.
We now take up this subject in the discussion:

> We put subjective evaluations at the centre of our considerations; we deal explicitly with researchers who are proponents of some theory (they have higher priors for the theory being true), researchers who suspect confounding variables (they assume lower theoretical risk), or those who remain doubtful if everything relevant was reported (they have higher uncertainty about theoretical risk) or even those who place greater value on incongruent evidence than others (they differ in their confirmation function).

:::reviewer
Preregistration does not give certainty about the theoretical risk, as the authors argue.
It gives more certainty in most cases at best, but it allows others to evaluate it.
This might sound like a subtle difference, but I think it matters.
:::

Agreed.

:::reviewer
The authors might reply that the did not really mean 'minimize uncertainty about the theoretical risk' but mainly meant that readers can judge for themselves what the Type 1 error of a test is (based on their own assumptions about what the most appropriate test is).
In that case, their argument does not differ from mine in Lakens (2019), and the authors should explain what their contribution adds.
:::

In the revision, we tried hard to be clearer that uncertainty always refers to the uncertainty of the reader.
However, being content with claiming that readers have to judge for themselves what the theoretical risk/severity of a test results in a weak argument for preregistration.
We go further and show that for all individual assessments, preregistration has a benefit and that if one is willing to assume some parameters, this benefit is striking.

:::reviewer
Point 2: It remains unconvincing why preregistration is useful for exploratory studies.
The authors seem to argue that even in exploratory studies preregistration will remove *some* uncertainty about the ability of a test to falsify a prediction.
That is true.
However, they do not argue why it would do so to an extent that makes preregistration useful in real life. If an action I do reduces my uncertainty from 98% to 94%, it has relatively reduced my uncertainty.
At the same time, it has done so in a manner that does not matter at all.
My point here is that the error rate in exploratory research is inflated above alpha% to an unknown extent.
A preregistration might reduce the inflation somewhat.
But the rate is quickly too high so that I wouldn't trust the claim anyway.
:::

Actually, we think this is one of the strengths of the paper.
With the mathematical analysis, we show exactly how much uncertainty, theoretical risk, and detectability result in how much confirmation, as seen in Figures 1 and 2.
The vague intuition of a "[Type 1 error] rate [that] is quickly too high so that I wouldn't trust the claim anyway" is replaced by a clear functional form that enables navigating tradeoffs involving exploration, post-hoc changes or detectability.

:::reviewer
If a claim is made with 10% lower Type 1 error rate than it would have been made when it was not preregistered is not useful - because I do not know what the error rate associated with the claim is.
:::

Our claim is that uncertainty is reduced, not theoretical risk directly.
We claim that there are exploratory methods with a well-known error rate.
In our argument, the difference between a t-test and a random forest is a matter of degree and not of kind.
A t-test may maintain its 5%-alpha error and a random forest may find a variable important 30% of the time.
For both, however, their behavior is mathematically analyzable, yielding a concrete expectation for their theoretical risk.

:::reviewer
Thus, preregistration in exploratory studies reduces the error rate inflation, but it does not make the error rate known.
Therefore, I do not believe preregistration in exploratory studies is useful, and the authors do not make an argument for it.
We do not want to reduce the error rate inflation in preregistration - we want to control it.
:::


Somehow, our argument seems to be flipped here.
An exploratory study has low theoretical risk.
A preregistered study has low uncertainty about theoretical risk.
A preregistered exploratory study is more persuasive than a study with the same degree of exploration that is not preregistered.

:::reviewer
If the error rate is not controlled, I do not know how severely a claim has been tested, and I can not use the claim to decide how to act as a scientist.
The authors seem happy with the fact that preregistration reduces the Type 1 error rate even in an exploratory study.
:::

No. 
As we argue in our manuscript, we are happy when uncertainty about theoretical risk is low.
A typical exploratory study is in our framework characterized by low theoretical risk (high Type 1 error rate).
Reducing uncertainty about its theoretical risk is still useful for scientific progress, in order to be better able to judge its theoretical risk.

:::reviewer
But I am not. Only claims with a known error rate are useful in science.
:::

Exactly. The error rate must be known (readers must be certain about them).
It may be high however (=exploratory).

:::reviewer
If the authors disagree, they should say how an unknown relative decrease in the Type 1 error rate to a final still unknown type 1 error rate is beneficial.
:::

We have a hard time following this reasoning.
Exploratory does not imply that theoretical risk is unknown.

:::reviewer
Point 3: The Type 2 error rate matters as much as the type 1 error rate I do not believe the relationship the authors draw between 'theoretical risk', 'risky predictions', 'evidence', and the 'probability of a hypothesis' is coherent.
I especially disagree with the authors that the posterior probability P(H\|E) is important in preregistration.
The authors say this probability is 'often used directly or indirectly as a measure of confirmation of a hypothesis'.
This is simply not true.
No one discusses posterior probabilities of hypotheses in psychological science.
:::

Neither is severity -- as an absolute measure of corroboration -- a common consideration in empirical psychological work.
However, the posterior probability is certainly the most classic measure of absolute confirmation in the philosophy of science.

:::reviewer
The authors then discuss an 'increase in firmness'.
Logically, this would be the Bayes factor, not the posterior probability of the hypothesis, that the authors are talking about.
The Bayes factor is a continuous measure of how much we update our belief.
If we want to argue for any function of preregistration from a Bayesian perspective, it must be focused on the Bayes factor.
The authors then say 'If a hypothesis is more probable to begin with, observing evidence in its favor will result in a hypothesis that is more strongly confirmed, all else being equal'.
:::

We explicitly consider the whole space of possible measures.
We tried to clarify this.
Quoting from the revised paper:

> The goal is therefore to reason about the space of possible measures researchers might apply.
> However, since any measure fulfilling the statistical relevancy condition increases monotonically with an increase in posterior probability $P(H|E)$, we might well take it to illustrate our reasoning.
> Later in the paper: Of course, the assigned probabilities and the distribution $Q$ vary from study to study and researcher to researcher (and even the measure of confirmation), but we can illustrate the effect of uncertainty with an example.

:::reviewer
This statement is not in line with the earlier statement about an 'increase in firmness'.
Of course, if a hypothesis is highly probable (the Stroop effect), any evidence will end up with a higher posterior probability that if the prior probability was low.
But the same evidence would lead to a greater 'increase in firmness' (or a higher Bayes factor) if the prior was lower.
The same data would lead to a much greater update in belief.
The authors dismiss the prior probability of the hypothesis as it 'is nothing our study design can change'.
However, this is not true.
Although the authors only discuss inflating the alpha level in their paper, one function of preregistration is to prevent HARKing, or hypothesizing after the results are true.
Of course, if one comes up with a hypothesis after looking at the data, the 'prior' is 1: We already know the hypothesis will be supported by the data.
:::

We included a section called "Hacking, harking, and other harms", which hopefully clarifies our position.

:::reviewer
HARKing is a major reason the field has moved towards preregistration, but the term 'HARKing' is not even mentioned in the article.
The reason for this is that the arguments the authors provide can not deal with HARKing.
This is obviously a limitation, compared to existing arguments for preregistration, and it is incorrectly ignored in the article.
As Mayo (2018) explains, the problem with HARKing is that it leads to a non-severe test:
The data have no probability of falsifying the claim.
This error-statistical perspective seems to me to be much more fruitful than a Bayesian approach.
:::

As stated above, we come to the same conclusions despite being inductionists.

:::reviewer
The authors then move on the P(E\|H), or power.
They argue 'while detectability is of great importance for study design, it is not directly relevant to the object of preregistration.' This is incorrect.
As I write in Lakens, 2019: "In line with the general tendency to weigh Type 1 error rates (the probability of obtaining a statistically significant result when there is no true effect) as more serious than Type 2 error rates (the probability of obtaining a non-significant result when there is a true effect), publications that discuss preregistration have often been more concerned with inflated Type 1 error rates than with low power.
However, in situations where researchers want to find a null effect low power is a bigger concern."
The idea of null-hacking is not crazy - sometimes a certain variable should *not* be significant to exclude alternative explanations. Therefore, preregistration certainly should have the function to prevent researchers from incorrectly claiming there is no effect, when there is an effect, by inflating the Type 2 error rate (or equivalently, by reducing the power).
:::

Absolutely. However, note that our H is always the research hypothesis, independent of the statistical H1 or H0.
If a researcher wants to provide evidence for a null hypothesis, this is their H, theoretical risk, then is related to power.
We tried to clarify this in our revision:

> In fact, it is entirely possible to assume the null hypothesis as a research hypothesis, as is commonly done in e.g., structural equation modelling.
Then, the roles of detectability, theoretical risk and type-I/II error rate switch, yet our line of reasoning stays the same.

We reference the ideas of frequentist null hypothesis testing to provide readers who are more familiar with this line of reasoning with a better intuition, not to claim equivalence.

:::reviewer
This aspect of Bayes' formula is therefore also incorrectly ignored in the article.
The authors then argue that P(E) is the only part of the formula that remains to be considered.
Decreasing P(E) can increase the posterior probability.
Of course, P(E) is a function both of the power and the Type 1 error rate.
But for some unclear reason, the authors here dismiss power, as they write "We have already noted that there is not much to be done about prior probability ((), and hence its counter probability (¬)), and that it is common sense to increase detectability (\|)."
But this latter statement dismisses the important role of power much too easily.
One could just as well say it is 'common sense' to reduce the Type 1 error rate.
Furthermore, once again, we do not 'increase' error rates, we *control* them.
:::

We are under the impression that two inconsistent definitions of *control* of error rates are used by the reviewer.
First, a definition in which control means the error rates are known (but then known by whom?).
Second, control means ensuring certain acceptable bounds of error rates are not exceeded.

:::reviewer
And while controlling them, we always have to consider resource constraints (Lakens, 2022).
Indeed, one might even say that we are always balancing Type 1 and Type 2 error rates, and that it would be sensible to move away from a fixed alpha level for all studies (Maier and Lakens, 2022).
In any case, the authors fail to adequately argue why power can be ignored, and the Type 1 error rate is important.
:::

Nowhere have we stated that power can be ignored for statistical inference; we just do not think that power is the main reason why preregistration is a good thing.
We agree that detectability is important; after all, it shows in every mathematical consideration we made that it is a vital consideration for a Bayesian.

:::reviewer
According to the error-statistical justification of preregistration (Mayo, 2018; Lakens, 2019) both are equally important.
At the very least, increasing the alpha level would also increase the statistical power, so the two can not even be seen as separate. 
:::

We wholeheartedly agree with the importance of being able to navigate these tradeoffs.
That is why we find a mathematical treatment of these ideas to be so important.
The Bayesian perspective allows us to ask, very concretely, how persuasive an idealized Bayesian thinker finds different configurations, including the ratio of theoretical risk and detectability.
We clearly say multiple times that these tradeoffs can not be optimized separately.
In the revised paper:

> As noted before, we strive for high detectability and high theoretical risk in planning, conducting, and analyzing studies.
> Maximizing one at the expense of the other is not necessarily beneficial for increasing epistemic value but depends on the specific function they apply to judge evidence and their specific location on the curve.
> One advantage of our framework is that researchers can employ it to balance the trade-off more effectively, assuming they are willing to make some simplifying assumptions.

:::reviewer
POINT 4: Terminology As terminology can always be fixed, this point is not as problematic to the reasoning as the previous 3.
The authors use the word 'theoretical risk' for the probability that a test of a hypothesis yields a true negative.
As the authors exclusively talk about the probability that statistical tests will yield a correct conclusion, 'theoretical risk' feels as a misnomer (statistical risk would be better, but severity might also be a good general term).
:::

Please note that our framework adressess the entire scientific procedure, of which a statistical test is only one piece.
It would cost our framework its generality if we reduced 'theoretical risk' to only a property of the statistical test (such as severity).

:::reviewer
True, in page 13 they give an example about theoretical risk that is actually theoretical risk - but this example has nothing to do with the remainder of the paper, as all three studies can have exactly the same alpha level!
:::

We are unable to follow this argument without more elaboration.

:::reviewer
They are talking about a low frequentist error rate in all their examples, and as 'theoretical risk' is the inverse of the false positive rate, it might be easier to just talk about the Type 1 error rate.
:::

This concern is conceptually very similar to the second-to-last concern.
The point of not using a Type 1 error rate is to differentiate the statistical test from the the researcher's scientific judgement.
Type 1 error rate is a property of a test under certain conditions.
Theoretical risk refers to the individual judgement of a researcher about the substantive hypothesis and the study design as a whole.

:::reviewer
The authors say they borrowed the term 'theoretical risk' from Meehl, but that he did not assign it to (¬\|¬).
The authors cite Kukla (1990), but after reading it, I am not convinced they are correct in equating 'theoretical risk' with the Type 1 error rate.
Theoretical risk means making an a-priori unlikely prediction if the theory was not true.
:::

This sentence sounds exactly like $P(\neg{}E|\neg{}H)$ to us.
The judgement about $P(\neg{}E|\neg{}H)$ is a-priori, is about "prediction" as a statement about evidence E, given that the theory is not true ($\neg{}H$).
One might argue, that it should be multiplied by $\neg{}$ perhaps? 
But that would not change any of the arguments.
In any case, we provide clear mathematical definitions for our terms.
If a reader does not agree with our terminology, they are free to term them differently.
In fact, this happens in reference to a finding we found in the meantime (Oberauer and Lewandowsky (2019)), which we added to the discussion.

> Whatever the difference in evaluating preregistration as a tool, maybe conceptually more profound is that @oberauerAddressingTheoryCrisis2019 conceptualizes "discovery-oriented research" differently than we do "exploratory".
They assume the same theoretical risk ($P(\neg E| \neg H) = .05$) and detectability ($P(E| H)) = .8$) in their calculation example as we do but assign different prior probabilities, namely $.06$ for discovery versus $.6$ for theory testing.
Then, they conclude that discovery-oriented researcher requires a much lower type-I error rate to control false positive in light of the low prior probability.
This runs counter to our definition of exploratory research having low theoretical risk.
Of course, we agree that low priors require more persuasive evidence; our disagreement, therefore, lies mainly in terminology.
They imagine discovery-oriented researchers to conduct experiments where they have low expectations that they obtain positive evidence ($.06 \cdot .8 + .94 \cdot .05 = 0.095$), but if they do, it raises the posterior significantly (from $.06$ to $.51$)
In our view, researchers who set out to explore a data set often find "something" (due to low $P(\neg E| \neg H)$); therefore, it should only slightly raise your posterior if they do.
On a substantive matter, we believe both kinds of research are common in psychology.
It is, therefore, mostly a disagreement on terminology.
This disagreement only highlights why using a mathematical framework to investigate such things is so useful and ultimately indispensable because we can clearly see where and how we differ in our reasoning.

:::reviewer
This could be, as Meehl would like, a range prediction. Or, in the example of the authors on page 13, it is a test without confounds.
In Lakens, 2019, on page 224, I explain how a two-sided test, one-sided test, and a range prediction all differ in how severe they test the hypothesis, even if they all have the same 5% alpha level ("Note that the three tests differ in severity even when they are tested with the same Type 1 error rate.").
:::

Yes, indeed, these could all have low theoretical risk.
We could include the disclaimer to the examples on p. 13 "Note that the three tests differ in theoretical risk even when they are tested with the same Type 1 error rate."

:::reviewer
This is another reason why I think the term 'theoretical risk' is not a good choice.
Indeed, based on the example on page 13, it seems the authors themselves confuse real theoretical risk with Type 1 error control.
However, it is better to use terminology that encompasses Type 1 and Type 2 errors (such as 'the probability that a test will falsify a hypothesis', or even better: the *severity* of a test).
:::

Different philosophies differ in their terminology.
Deborah Mayo has good reasons to differentiate Type 1 error rate from severity, as do we.

:::reviewer
Using the term 'severity of the test' would also embed the work more strongly in philosophy of science (and specifically the work of Mayo, as she builds on Popper).
:::

This would embed us in a different philosophy of science.
Popper is the most famous anti-inductionist; we deliver an inductionist account.

:::reviewer
Mayo's work has been exactly on this topic, and it is a shame it has not been discussed, even though she discusses the problem of p-hacking and Type 1 error rate inflation extensively in her 2018 book.
The authors write "The amount of knowledge researchers gain from a particular study concerning the verisimilitude of a specific theory is what we call epistemic value".
This is an unfortunate term, as epistemic values are already used in philosophy of science to refer to qualities such as predictive accuracy, internal coherence, external consistency, unifying power, and fertility - see Elliott, K. C. (2022). Values in Science (1st ed.). Cambridge University Press. <https://doi.org/10.1017/9781009052597>.
I would pick another terminology.
In our own work published in Psychological Methods we called this "the marginal gain in expected utility (or usefulness) of scientific claims". (Isager, P. M., van Aert, R. C. M., Bahník, Š., Brandt, M. J., DeSoto, K. A., Giner-Sorolla, R., Krueger, J. I., Perugini, M., Ropovik, I., van 't Veer, A. E., Vranka, M., & Lakens, D. (2023).
Deciding what to replicate: A decision model for replication study selection under resource and knowledge constraints. Psychological Methods, 28(2), 438-451. [https://doi.org/10.1037/met0000438)/](https://doi.org/10.1037/met0000438)/){.uri}.
There are probably other existing terms for the same idea in the literature. The authors discuss P(E\|H) as 'detectability' or the probability of detecting evidence if a hypothesis holds.
This is thus simply statistical power, or the true positive rate.
I think the term detectability is not particularly useful, as it is more strongly associated with signal detection theory - but that is not what the authors mean with the term, as they admit the term is simply statistical power when applied to hypothesis testing (and I don't know what else it could be applied to).
:::

Hypothesis testing and scientific inference are different things.
A test might be significant, but if a researcher suspects strongly enough that there might be a problem, they may decide not to believe the hypothesis.

:::reviewer
Minor points: The authors say "We note these relations to underline that the Bayesian rationale we apply here is able to reconstruct many commonly held views on riskiness and epistemic value." when they relate a property of their Bayesian model with Popper's work.
:::

No, we relate our work to beliefs we think are commonly held.
Researchers find the idea of a risky test appealing.
Popper gives one explanation for this appeal, but we happen to adopt a different philosophical view here.

:::reviewer
However, this is misleading.
Popper would never agree with the approach in this paper, as he does not think subjective prior or posterior beliefs should play a role in scientific claims.
According to Popper, verisimilitude is not about belief. Popper, p. 434: "As to degree of corroboration, it is nothing but a measure of the degree to which a hypothesis h has been tested, and of the degree to which it has stood up to tests.
It must not be interpreted, therefore, as a degree of the rationality of our belief in the truth of h".
The author's proposal violates the philosophical work of Popper at it's very core, and is incommensurable.
:::

As stated above, if Popper happened to agree, we indeed would have to rethink our arguments.

:::reviewer
Just because the authors also need to rely on the likelihood ratio, does not mean it 'reconstructs' Popperian philosophy of science.
:::

Which we never claimed.

:::reviewer
It is not clear what the researchers mean with 'Researchers judge the evidence for or against a hypothesis rationally'. Researchers often differ in whether a study is evidence for or against a hypothesis, often because they disagree on auxiliary assumptions (e.g., A thinks X is a good measure of Y, but B does not think X measures Y).
Are these two researchers irrational? Are the authors talking about some utopian world, in which case this assumption seems violated at least for now.
:::

No, these researchers are not irrational according to our definition.
For a researcher to be rational, they have to be internally consistent.
They must not change their process of weighing evidence just because they do not like the outcome.
We take this assumption from the literature of the Bayesian philosophy of science and cite it appropriately.

:::reviewer
The authors admit they are talking about an 'ideal' estimation process.
:::

We meant "ideal" similar to how a physicist would use the term in the context of "ideal gas".

:::reviewer
But in an ideal estimation process we would also have infinite data.
Maybe I am being too pragmatic, but shouldn't we discuss how research is actually done?
:::

We agree to assume infinite data would go a little too far into wishful thinking.
Also, with infinite data, it stops being fun to apply Bayesian reasoning since there is no uncertainty to deal with.

:::reviewer
The authors write "Indeed, limiting the type-I error rate is commonly stated as the central goal of preregistration (Nosek et al., 2018; Oberauer, 2019; Rubin, 2020)."
It might be commonly stated. But then, people also commonly state the incorrect definition of a p-value.
Instead, the authors could cite a better paper, like my own: "The goal of preregistration is not simply to control the Type 1 error rate in hypothesis tests, but to prevent researchers from non-transparently reducing the capacity of the test to falsify a prediction in general."
I worked quite hard on conceptually clarifying the role of preregistration, correcting exactly the kind of misconception the authors also respond against.
:::

We appreciate this important work.
Yet we believe that this is not the only useful perspective on the utility of preregistration.

:::reviewer
The authors write "First, according to most interpretations of null hypothesis testing, the absence of a significant result should not generally be interpreted as evidence against the hypothesis (Mayo, 2018, p. 5.3)."
This is generally true, but of course null effects often *are* evidence for the null - as would be clear if an equivalence test was performed, or a Bayes factor or likelihood ratio was computed.
:::

With the following statement, we hoped to make clear that there are many statistical procedures that can be investigated through the lens we adopted here:

> Please note that our decision to adopt a Bayesian philosophy of science does not make assumptions about the statistical methods researchers use.

:::reviewer
The authors argue (page 15) theoretical risk also encompasses factors outside the statistical realm.
I agree, and say so in Lakens, 2019: "Making very narrow range predictions is a way to make it statistically likely to falsify your prediction if it is wrong.
But the severity of a test is determined by all characteristics of a study that increases the capability of a prediction to be wrong, if it is wrong.
If a researcher predicts that an effect will only be observed under a very specific set of experimental conditions that all follow from a single theory, it is possible to make theoretically risky predictions."
Line 425: There are 3 arguments, not 2.
:::

:::reviewer
\label{pt:change-start}

\label{pt:harking}
Conclusion: Again, I appreciate the authors digging in to the justification of preregistration, but I think there are some important things missing in this version of the manuscript.
How can the article be improved? The authors should address harking.
:::

We included a section called "Hacking, harking, and other harms", which hopefully makes our thinking on these thinkgs clearer.

:::reviewer
\label{pt:exploratory}
They should explain better why they think preregistration of exploratory studies is useful.
:::

We hope that the improvements that we made to better explain the topics of subjectivity and uncertainty make clearer that theoretical risk and uncertainty about theoretical risk are two dimensions.

:::reviewer
\label{pt:terminology}
They should improve terminology.
:::

We take the criticism of the terminology seriously.
We have opted to stick with most of the terms as we defined them initially, as we did with good reasons, we now back them up with additional references.


:::reviewer
\label{pt:severity}
The should better integrate their ideas with existing ideas, especially Mayo's work on severity, but at least engage with the arguments for preregistration in the literature more than a somewhat strawman account of minimizing the type 1 error rate.
:::

We have discussed Mayo's Severity to show that these approaches fundamentally differ, and we have significantly extended the discussion to include literature that also uses a Bayesian rationale to investigate what methods researchers should employ.
Additionally, we substantiated the "strawman" further because we believe that the minimizing vs communicating researchers' degrees of freedom confusion is at the center of many debates around preregistration. 

:::reviewer
\label{pt:change-end}
\label{pt:nullhacking}
They should also address 'null-hacking'.
:::

We have merely mentioned this topic in passing since it is conceptually only difficult if the null hypothesis has a special position, as in frequentist reasoning.
However, we significantly expanded on the topic of "hacking" in general, under which this topic falls for us.

:::reviewer
After these changes, I am not sure what their argument will end up looking like, but I would hope the authors explore their ideas further.
Sorry for citing myself, no need to cite me - others often make the same points I make, so pointing you to my own papers is mainly just easier for myself. Lakens, D. (2019). The value of preregistration for psychological science: A conceptual analysis. Japanese Psychological Review, 62(3), 221-230. <https://doi.org/10.24602/sjpr.62.3_221Lakens, D. (2022). Sample Size Justification. Collabra: Psychology, 8(1), 33267. <https://doi.org/10.1525/collabra.33267Maier, M., & Lakens, D. (2022). Justify Your Alpha: A Primer on Two Practical Approaches. Advances in Methods and Practices in Psychological Science, 5(2), 25152459221080396. <https://doi.org/10.1177/25152459221080396>
Signed,
Daniel Lakens
:::

We thank Daniel Lakens for the review and for allowing us to share it!

# James Brockmole


:::reviewer
Dear Mr. Peikert:

I have now read Manuscript PBR-TR-23-119 entitled "Why does preregistration increase the persuasiveness of evidence? A Bayesian rationalization" that you submitted to Psychonomic Bulletin & Review.
You offer a new perspective on an important issue, and I truly enjoyed reading through your arguments for the role of preregistration in reducing uncertainty about theoretical risk.
:::

We are thankful for the kind words.

:::reviewer
That said, this is more of a think piece than a theoretical review, and the value of assessing theoretical risk remains hypothetical.
I think you would agree that your examples are highly schematic and serve to illustrate the principles that you are proposing as opposed to reporting specific examples of how these principles can improve psychological research.
Perhaps it would be possible to compare a range of studies with and without preregistrations to show how the act of preregistration helps clarify the reported results even when the preregistration does not report the level of detail required for a truly confirmatory study.
Given your broad scope, the reviewed studies would ideally come from a variety of research domains.
In other words, perhaps it is possible to put your ideas through a few test runs to see how they can be applied when all the messy details of specific research endeavors are considered.
:::

We are sure this would be possible and enlightening, but also way beyond the scope of the current paper.
Out goal is more modest: we want to provide a Bayesian justification of pre-registration that 1. clearly articulates the aims of a preregistration and 2. describes how preregistration achieves these goals.
As a result, it eliminates common missconceptions about preregistrations, for example, that you are not allowed to deviate under any circumstances.
To achieve these goals, our examples are purposely schematic, as we want them to apply to all empirical sciences.

:::reviewer
In my view, theoretical reviews propose a new set of ideas *and* systematically apply the new ideas to existing findings to establish new discoveries or bolster conclusions with more rigorous methods.
I don’t see the second element here, and I don’t see how getting reviewer comments would change that impression.
:::

We agree. Empirically establishing our perspective on preregistration as "correct" would be very difficult.

:::reviewer
ACTION: I am rejecting the manuscript.
Although a desk reject seems like a very negative outcome, I want to again emphasize that I found your analysis creative and valuable.
I just don’t think a PBR theoretical review is the appropriate outlet.

Thank you for considering Psychonomic Bulletin & Review for the publication of your research.

Sincerely, Jeff Starns Action Editor, Psychonomic Bulletin & Review

Editor: James Brockmole
:::

# References
